\documentclass[a4paper]{article}
\usepackage{hyperref}
%\VignetteIndexEntry{Introduction to grid}
%\VignettePackage{WIDataRipper}
%\VignetteDepends{RCurl}
%\VignetteDepends{rjson}

% Definitions
\newcommand{\rlan}{{\tt R}}
\newcommand{\widg}{{\tt WIDataRipper}}
\newcommand{\Rcurl}{{\tt RCurl}}
\newcommand{\spPACKAGE}{{\tt sp}}
\newcommand{\rjson}{{\tt rjson}}
\newcommand{\code}[1]{{\tt #1}}
\setlength{\parindent}{0in}
\setlength{\parskip}{.1in}
\setlength{\textwidth}{140mm}
\setlength{\oddsidemargin}{10mm}
\SweaveOpts{keep.source=TRUE}

\title{\widg{}: A simple quick way of getting NSW Hydrological data from within \rlan.}
\author{Jason Lessels}

\begin{document}
\maketitle
\pagebreak
\tableofcontents
\pagebreak
\section{Introduction}
\widg\ is designed to provide a simple and quick method to get data from the NSW water info website \href{http://waterinfo.nsw.gov.au/water.shtml?ppbm=SURFACE_WATER&rs&3&rskm_url}{(Real-time water data)}. The main aim of this package s to provide the ability of the direct importation of data from the web server. Additional functions have been added to the package to allow for more complicated searches and meta data queries.
%% Set up the echo parameters

\section{Installation of \widg}
To install the \widg\ package, both the \Rcurl\ and the \rjson\ packages are required. 
<<eval=FALSE>>=
install.packages("RCurl")
install.packages("rjson")
@
To install the \widg\ package the \href{http://r-forge.r-project.org}{R-Forge} repository must be provided.
<<eval=F>>=
install.packages("WIDataRipper", repos="http://R-Forge.R-project.org")
@
\section{The main functons.}
\widg\ has several functions designed to be used in conjunction to get the desired data off the server. The main work flow assumes the user knows the desired site number. In upcoming releases a site name query will be added. My current work involves a site near Coolac, south west of Canberra. The following examples will provide an example of how to use this package to obtain desired data. \\
The first stage in obtaining the data, is to first get some meta-data about the site. Using the function \code{getSiteInfo}.
<<>>=
library(WIDataRipper)
cat(paste(strwrap(getSiteInfo(410044), width = 70), collapse = '\\n'))
#writeLines(strwrap(capture.output(getSiteInfo(410044))))
@
With the results of this function, we now have the site location and the elevation and any comments about the site, and the data recording process at the site. The next important piece of meta-data is the available variables at the site and the length of time they have been collected for. However, due to the setup of the server, there are potentially several data sources for each site. Below the available data sources for the site are obtained.
<<>>=
getSiteDataSources(410044)
@
From my current understanding data source `PROV' are any samples that have not undergone proper quality coding. That is to say that no one from the department has looked at these values in any real detail. There is two important things to note about this. There is overlap between some of these values from each data source. The second thing to note is that the latest bleeding edge values from each site tend to be within the `PROV' data source.

The next stage is to find out what variables are within each data source for the site. The first time will be for the `A' data source.
<<>>=
getSiteVariables(410044,data_source="A")

@
The next enquiry will be for the `PROV' data source.
<<>>=
getSiteVariables(410044,data_source="PROV")
@
The main difference between the two results are the starting and ending dates. However The `PROV' data source provides a few additional variables: Logger Battery Voltage and Inst. Salinity.

With all the above meta-data gathered it is now possible to get the desired data. The method for this is the following
<<keep.source=TRUE>>=
streamHeight <- getData(site_number=410044,start_time="20110101000000",
end_time="201102010000000",interval="day",variable_number=100)
@
\begin{figure}
\begin{center}
<<label=fig1,fig=TRUE>>=
plot(streamHeight$data[,1:2],type="l")
@
\end{center}
\caption{An example of daily stream height for one month}
\label{fig:one}
\end{figure}

And the EC can also be obtained.
\begin{figure}
\begin{center}
<<echo=F,fig=TRUE,label=fig2>>=
streamEC <- getData(site_number=410044,start_time="20110101000000",
end_time="201102010000000",interval="day",variable_number=2010)
plot(streamEC$data[,1:2],type="l")
@
\end{center}
\caption{An example of daily EC for one month}
\label{fig:two}
\end{figure}


 
\section{The added bonus functions}
OK, so that is the it with the main functions now on to some other functions. Mainly designed for searching the data base. There is one additional function \code{getLatest} that provides the ability to get the last 7 days of observations. The options with this function are limited and is mainly designed to help users keep track of what it happening at their study site. It could be setup to run when R is started every day.

\begin{figure}
\begin{center}
<<label=fig3,fig=TRUE>>=
latest <- getLatest(410044,variable_number=100)
plot(latest$data[,1:2],type="l")
@
\end{center}
\caption{An example of the latest function.}
\label{fig:three}
\end{figure}

The remainder of the functions within the package provide the ability to search the data base. The function \code{getAllSites} queries the server for every site in the data base. The function returns a data.frame with every name of every site. There are a little over 3000 sites, so one might want to just believe me that it works.

\subsection{Sites within geographical boundaries}
There are currently two functions that allow for searching for sites within geographical boundaries \code{getSitesWithinCircle} and \code{getSitesWithinrectangle}. Both functions retrieve a list formatted in the same style as the \code{getSiteInfo} function. But contain all sites with either a specified circle or rectangle. 

\section{Further upgrades}
In upcoming releases two additional functions will be added. A search by site name and a search by town name.  An export function for the geographical functions will be added allowing for the conversion to a \code{SpatialPointsDataFrame} from the \spPACKAGE\ package. I hope to include the ability to search for all sites within a given polygon, that accepts a \spPACKAGE\ formatted object.

Other search queries are possible, but I do not have any other needs personally. I am happy to add additional search queries on request.
\end{document}